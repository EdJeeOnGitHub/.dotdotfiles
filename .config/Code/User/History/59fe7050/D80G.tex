\documentclass{article}
\usepackage{amsmath}
\usepackage[]{amsfonts}
\newcommand{\indep}{\perp \!\!\! \perp}

\begin{document}
  
\subsubsection*{Setup}
Suppose we have binary $Y$ and $X$ and we want to know the effect of $X$ on $Y$ 
after conditioning on some propensity score which is a function of covariates 
$C$. The authors propose the following joint likelihood:

\begin{align*}
    L(Y, X | C, \theta) &= \prod^n_{i = 1} \{
        g_x^{-1}(C_i \gamma)\}^{X_i} \{
            1 - g_x^{-1}(C_i\gamma)\}^{1 - X_i} \times \\
            &\{
                g_y^{-1}(\xi_0 + \beta X_i + \xi h(\gamma, C_i) + \delta C^+_i)
                \}^{Y_i} 
                \{1 -
                g_y^{-1}(\xi_0 + \beta X_i + \xi h(\gamma, C_i) + \delta C^+_i)
                \}^{1 - Y_i} 
\end{align*}

They estimate the propensity score in the first line $g_x^{-1}(C_i\gamma)$ and 
condition on it in the second line by adding it as a covariate in the mean vector 
that is subsequently transformed by the link function $g_y^{-1}$.
They argue that because Bayesian inference is joint and $\gamma$ appears in this likelihood in both the treatment 
model and outcome model there is ``model feedback''.

\subsubsection*{Another Attempt}

Let's rewrite the system of equations as:
\begin{align*}
    Y &= \mathbb{I}\{\beta X +  C_i^+ \delta + \varepsilon_1 > 0\} \\
    X &= \mathbb{I}\{ C_i\gamma + \varepsilon_2 > 0 \} \\
    \begin{pmatrix}
        \varepsilon_1 \\
        \varepsilon_2
    \end{pmatrix} &\sim N\left(0, \begin{pmatrix}
        1 & \rho \\
        \rho & 1
    \end{pmatrix}\right)
\end{align*}
Where we've normalised variance to 1 to make things easier (and this is only 
identified up to scale anyway). Now let's think about the likelihood again:
\begin{align*}
    Pr(Y = y, D = d) &= \left\{
        Pr(Y = 1 | D = 1) Pr(D = 1)
    \right\}^{YD} + \\
&\left\{
        Pr(Y = 0 | D = 1) Pr(D = 1)
    \right\}^{(1 -Y)D} + \\
&\left\{
        Pr(Y = 1 | D = 0) Pr(D = 0)
    \right\}^{Y(1 - D)} + \\
&\left\{
        Pr(Y = 0 | D = 0) Pr(D = 0)
    \right\}^{(1 -Y)(1 - D)} 
\end{align*}

Let's focus on $Pr( Y = 1 | D = 1)$, using conditional normality we can always rewrite:
\begin{align*}
    \varepsilon_1 &= b\varepsilon_2 + u
\end{align*}
where $u \indep \varepsilon_2$ and $b = \frac{cov(\varepsilon_1, \varepsilon_2)}{var(\varepsilon_2)} = \rho$. 
This gives, for $\rho > 0$:
\begin{align*}
    Pr(Y = 1 | X = 1, C) &= \frac{Pr(\beta + \delta C^+ + \rho \varepsilon_2 + U > 0 \cap C_i \gamma + \varepsilon_2  > 0)}{
    Pr(C_i \gamma + \varepsilon_2 > 0)} \\
    &= \frac{Pr(\varepsilon_2 > \max \{ - \frac{\beta + \delta C^+ + U}{\rho}, -C_i \gamma\})}{
        Pr(\varepsilon_2 > -C_i \gamma)
    } \\
    &= \frac{1}{\Phi(C_i \gamma)} \int
    Pr(\varepsilon_2 > \max \{ - \frac{\beta + \delta C^+ + u}{\rho}, -C_i \gamma\}) f(u) du
\end{align*}


where $f(u)$ is going to be some transformed normal density as it's the error from a projection 
of one normal variable on another correlated normal variable. Well, $\varepsilon_2$ is 
normal so:

\begin{align*}
    &= \frac{1}{\Phi(C_i \gamma)} \int
    \Phi\left( \min \{  \frac{\beta + \delta C^+ + u}{\rho}, C_i \gamma\}\right) f(u) du
\end{align*}


Note that if $\rho < 0$ then we get:

\begin{align*}
    &= \frac{1}{\Phi(C_i \gamma)} \int
    Pr(-C_i \gamma < \varepsilon_2 <  \frac{\beta + \delta C^+ + u}{\rho}) f(u) du \\
    &= \frac{1}{\Phi(C_i \gamma)} \int \left(
    \Phi\left(\frac{\beta + \delta C^- + u}{\rho}\right) - \Phi(-C_i \gamma)\right) f(u) du 
\end{align*}

In the Heckman model $\rho < 0, \rho > 0$ end up looking identical and my guess is 
that if you integrate enough things by parts the same will be true here. We'd also 
expect $f(u) \Phi(\cdot)$ to integrate to something approximately normal after 
manipulation so the nasty integral will probably disappear.
\subsubsection*{Some Comparisons}
If we compare this to the likelihood presented originally it's clear that we need 
to do more than just stick in the propensity score to the link function. The 
propensity score is great but it's not going to solve 
problems of model misspecification. So what have the authors modelled?
\begin{align*}
    Pr(Y = y, D = d) &=  Pr(Y = y | Pr(D = 1) = p_d) \times Pr(Pr(D = 1) = p_d) + ...\\
    &= Pr(\beta X + C_i^+\delta + \varepsilon_1 > 0 | Pr(D = 1) = p_d) \times ...  + ...\\
    &\neq Pr(\beta X + C_i^+ \delta + \gamma p_d + \varepsilon_1 > 0 | Pr(D =  1) = p_d) \times ... + ...
\end{align*}
In fact, we know that:
\begin{align*}
    Pr(Y = y, D = d) &=  Pr(Y = y | Pr(D = 1) = p_d) \times Pr(Pr(D = 1) = p_d) + ...\\
    &= Pr(\beta X + C_i^+\delta + \varepsilon_1 > 0 | Pr(D = 1) = p_d) \times ... + ... \\
    &= Pr(\beta X + C_i^+\delta + \varepsilon_1 > 0 | Pr(C_i \gamma + \varepsilon_2 > 0) = p_d) \times ...  + ...\\
    &= Pr(\beta X + C_i^+\delta + \varepsilon_1 > 0 | Pr(C_i \gamma + \varepsilon_2 > 0) = p_d) \times ...  + ...\\
    &= Pr(\beta X + C_i^+\delta + \varepsilon_1 > 0 | Pr(\varepsilon_2 > -C_i \gamma) = p_d) \times ...  + ...\\
    &= Pr(\beta X + C_i^+\delta + \varepsilon_1 > 0 | \varepsilon_2  = \Phi^{-1}_{-C_i\gamma}\left(p_d\right)) \times ...  + ...\\
    &= Pr(\beta X + C_i^+\delta + \rho \varepsilon_2  + U > 0 | \varepsilon_2  = \Phi^{-1}_{-C_i\gamma}\left(p_d\right)) \times ...  + ...\\
    &= Pr(\beta X + C_i^+\delta + \rho\Phi^{-1}_{-C_i\gamma}\left(p_d\right))  + U > 0 | \varepsilon_2  = \Phi^{-1}_{-C_i\gamma}\left(p_d\right)) \times ...  + ...\\
\end{align*}




In fact, 
from Heckman we know that if $g_y^{-1} = I(\cdot)$ i.e. the linear probability model
then $h(\gamma, C_i) = \rho \frac{\phi(-C_i\gamma)}{1 - \Phi(C_i \gamma)}$ for $X = 1$ 
and $-\rho \frac{\phi(-C\gamma)}{\Phi(C_i \gamma)}$ for $X = 0$. This is also known 
as the control function approach to IV.

\subsubsection*{IV}
Another way to think about this: in binary treatment, binary outcome IV then $\hat{X}$ 
estimated in the first stage is literally the  propensity score. When we do TSLS 
we replace $X$ in the second stage with $\hat{X}$ and interpret the coefficient on it. 
We certainly don't run OLS of $Y$ on $X, \hat{X}$ and interpret the coefficient on 
$X$ - the FWL theorem tells us that this is equivalent to regressing $X$ on $\hat{X}$ and 
 the coefficent will be all the components of $X$ not explained by the instrument, 
 literally the endogenous variation.

\end{document}