\documentclass{exam}
\usepackage{amsmath}
\usepackage{amsfonts}
\newcommand{\indep}{\perp \!\!\! \perp}
\printanswers
\begin{document}
    


\begin{questions}
    
\question If $X_n \xrightarrow{p} 0$ then $E[X_n] \rightarrow 0$
\begin{solution}
False. $X_n = n$ with probability $1/n$ and 0 with prob $1 - 1/n$,  $X = 0$
\end{solution}



\question If $X_1,...,X_n$ are an i.i.d. sequence of random variables with $0 < \text{Var}(X_i) < \infty$ then 
$P(Z_n = 1) \rightarrow 1/2$ with $Z_n = \mathbb{I}\{\bar{X}_n > E[X_i]\}$


\begin{solution}
    \begin{align*}
    P(Z_n = 1) &= P(\bar{X}_n - E[X_i] > 0) \\
    \bar{X}_n - E[X_i] &> 0 \\
    \frac{
        \sqrt{n}(\bar{X}_n - E[X_i])
    }{
        \sqrt{\text{Var}(X)}
    } &> 0 
    \end{align*}
    We know:
    \begin{align*}
    \frac{
        \sqrt{n}(\bar{X}_n - E[X_i])
    }{
        \sqrt{\text{Var}(X)}
    } \xrightarrow{d} N(0, 1)
    \end{align*}

    $\implies P(Z_n = 1) \rightarrow 1/2$.

\end{solution}




    \question If $E_{Y | X}[Y | X ] = E_Y[Y] \implies X \indep Y$.

    \begin{solution}
        
    False. 
    \begin{align*}
        X &= \cos \theta, Y = \sin \theta, \theta \sim U(0, 2\pi)
    \end{align*}

    For any $x \in [-1, 1]$:

    \begin{align*}
        Pr(Y = \sqrt{1 - x^2}) &= Pr(Y = - \sqrt{1 - x^2}) = 0.5 \\
        \implies E[Y | X = x] = 0
    \end{align*}
    and $E[Y] = 0$.

    $Pr(X > 3/4) > 0, Pr(Y > 3/4)$ need $Pr(X, Y) = Pr(X)Pr(Y)$.
    But $Pr(X > 3/4, Y > 3/4) = 0 \neq Pr(X > 3/4)Pr(Y > 3/4)$. (show graphically).


    Another example:

    \begin{align*}
        X \sim  N(0, 1), Y \sim N(0, \exp(X))
    \end{align*}

    $E[Y | X] = 0$ but $Pr(X > x, Y > y) \neq Pr(X > x) Pr(Y > y)$


    Finally, from pset 1, 7c:
    Consider $X\sim\mathcal{N}(0, 1)$ and $Y = X^2$. Then

        \begin{align*}
            E[X\vert Y] & = E[X\vert Y, X > 0] P(X > 0) + E[X\vert Y, X \leq 0] P(X \leq 0) \\
            & =  \sqrt{Y} P(X > 0) +\left(-\sqrt{Y}\right) P(X \leq 0) \\
            & =  \sqrt{Y} \frac{1}{2} -\sqrt{Y} \frac{1}{2} \\
            &=0
        \end{align*}

        But for example, $P(X\leq 0\vert Y = 0) = 1$ but $P(X\leq 0) = 0.5$. 
    \end{solution}


    \question If an estimator is unbiased then it must be consistent estimator for 
    $E[X_i]$ where $X_i$ iid.

    \begin{solution}
        False.

        Let $\hat{\mu} = \mathbb{I}\{i = 1\}X_i$.

        \begin{align*}
            E[\hat{\mu}] &= E[\mathbb{I}\{i = 1\}X_i]  \\
            &= E[X_1 + 0 + 0 + ...] \\
            &= E[X_1] = E[X_i]
        \end{align*}

        Since $X_i$ iid. However, as $n \rightarrow \infty$:

        \begin{align*}
            \hat{\mu} &= X_1 + 0 + ... \\
            Pr(|X_1 - E[X_i]| > \varepsilon) > 0
        \end{align*}
    \end{solution}


    \question If an estimator is consistent then it must be an unbiased estimator 
    for $E[X_i]$ where $X_i$ iid.


    \begin{solution}
    False. Consider the mditerm question 2 with $\delta < 1$.        



    \end{solution}


	\question The following function is a valid p.d.f. on $[-\infty, \infty]$:
		\[    f(x) = \max(0, 1 - 2 |x| ). \]

		\begin{solution}
		    False. The probability density function doesn't integrate to one.
		\end{solution}


    % \question If $BLP(Y | X) = \alpha + X \beta$ then $BLP(Y | 1/X) = \alpha + \frac{1}{\beta} X$

    \question
    $\mathbb{E}[u|X] {=} 0 \Rightarrow \mathbb{E}[Xu] {=} 0$.
    \begin{solution}
    True. Law of iterated expectations. $$\mathbb{E}[X u] = \mathbb{E}[\mathbb{E}[X u|X]] = \mathbb{E}[X \cdot \mathbb{E}[u|X]] = 0$$
    \end{solution}



\end{questions}









\end{document}